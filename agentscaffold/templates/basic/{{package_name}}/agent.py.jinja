"""Agent implementation for {{agent_name}}."""

import os
import json
import logging
import asyncio
from typing import Dict, Any, List, Optional, Union
# Import Pydantic
from pydantic import BaseModel, Field, ConfigDict


from agentscaffold.agent import BaseAgent, AgentInput, AgentOutput

class {{agent_class_name}}Input(AgentInput):
    """Input model for {{agent_class_name}}."""
    # Add any additional input fields specific to this agent
    pass

class {{agent_class_name}}Output(AgentOutput):
    """Output model for {{agent_class_name}}."""
    # Add any additional output fields specific to this agent
    pass

class {{agent_class_name}}(BaseAgent):
    """
    {{agent_class_name}} Agent implementation.
    {{description}}
    """
    
    def __init__(
        self,
        name: str = "{{agent_name}}",
        description: str = "{{description}}",
        **kwargs
    ):
        """
        Initialize the {{agent_class_name}} agent.
        
        Args:
            name: Name of the agent
            description: Description of the agent
            **kwargs: Additional configuration options
        """
        # Initialize logger before super().__init__
        self.logger = logging.getLogger(f"agent.{self.__class__.__name__}")
        self.logger.setLevel(logging.INFO)
        
        # Add console handler if not already added
        if not self.logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)
            
        super().__init__(
            name=name,
            description=description,
            **kwargs
        )
        
        # Set input and output classes
        self.input_class = {{agent_class_name}}Input
        self.output_class = {{agent_class_name}}Output
        
        self.logger.info(f"Initialized {{agent_class_name}} agent")
    
    def _init_llm(self):
        """Initialize LLM provider."""
        {% if llm_provider == "none" %}
        self.logger.info("No LLM provider configured")
        return None
        {% elif llm_provider == "openai" %}
        self.logger.info("Initializing OpenAI provider")
        try:
            from openai import OpenAI
            api_key = os.environ.get("OPENAI_API_KEY")
            if not api_key:
                self.logger.warning("OpenAI API key not found")
                return None
            
            return OpenAI(api_key=api_key)
        except ImportError:
            self.logger.warning("OpenAI package not installed")
            return None
        {% elif llm_provider == "anthropic" %}
        self.logger.info("Initializing Anthropic provider")
        try:
            import anthropic
            api_key = os.environ.get("ANTHROPIC_API_KEY")
            if not api_key:
                self.logger.warning("Anthropic API key not found")
                return None
            
            return anthropic.Anthropic(api_key=api_key)
        except ImportError:
            self.logger.warning("Anthropic package not installed")
            return None
        {% elif llm_provider == "ollama" %}
        self.logger.info("Initializing Ollama provider")
        try:
            import httpx
            base_url = os.environ.get("OLLAMA_BASE_URL", "http://localhost:11434")
            return {"client": httpx.AsyncClient(), "base_url": base_url}
        except ImportError:
            self.logger.warning("httpx package not installed")
            return None
        {% else %}
        self.logger.info("Using default LLM provider")
        return None
        {% endif %}
    
    def _init_search(self):
        """Initialize search provider."""
        {% if search_provider == "none" %}
        self.logger.info("No search provider configured")
        return None
        {% elif search_provider == "brave" %}
        self.logger.info("Initializing Brave Search provider")
        try:
            import httpx
            api_key = os.environ.get("BRAVE_API_KEY")
            if not api_key:
                self.logger.warning("Brave API key not found")
                return None
            
            return {"client": httpx.AsyncClient(), "api_key": api_key}
        except ImportError:
            self.logger.warning("httpx package not installed")
            return None
        {% elif search_provider == "browserbase" %}
        self.logger.info("Initializing BrowserBase provider")
        try:
            import httpx
            api_key = os.environ.get("BROWSERBASE_API_KEY")
            if not api_key:
                self.logger.warning("BrowserBase API key not found")
                return None
            
            return {"client": httpx.AsyncClient(), "api_key": api_key}
        except ImportError:
            self.logger.warning("httpx package not installed")
            return None
        {% elif search_provider == "google" %}
        self.logger.info("Initializing Google Search provider")
        try:
            from googleapiclient.discovery import build
            api_key = os.environ.get("GOOGLE_API_KEY")
            cse_id = os.environ.get("GOOGLE_CSE_ID")
            if not api_key or not cse_id:
                self.logger.warning("Google API key or CSE ID not found")
                return None
            
            return build("customsearch", "v1", developerKey=api_key)
        except ImportError:
            self.logger.warning("google-api-python-client package not installed")
            return None
        {% else %}
        self.logger.info("Using default search provider")
        return None
        {% endif %}
    
    def _init_memory(self):
        """Initialize memory provider."""
        {% if memory_provider == "none" %}
        self.logger.info("No memory provider configured")
        return None
        {% elif memory_provider == "chromadb" %}
        self.logger.info("Initializing ChromaDB provider")
        try:
            import chromadb
            from chromadb.utils import embedding_functions
            
            # Initialize embedding function
            openai_api_key = os.environ.get("OPENAI_API_KEY")
            embedding_function = None
            if openai_api_key:
                embedding_function = embedding_functions.OpenAIEmbeddingFunction(
                    api_key=openai_api_key,
                    model_name="text-embedding-3-small"
                )
            
            client = chromadb.Client()
            collection = client.get_or_create_collection(
                name="{{package_name}}_memory",
                embedding_function=embedding_function
            )
            return collection
        except ImportError:
            self.logger.warning("chromadb package not installed")
            return None
        {% elif memory_provider == "supabase" %}
        self.logger.info("Initializing Supabase provider")
        try:
            from supabase import create_client
            
            url = os.environ.get("SUPABASE_URL")
            key = os.environ.get("SUPABASE_KEY")
            if not url or not key:
                self.logger.warning("Supabase URL or key not found")
                return None
            
            return create_client(url, key)
        except ImportError:
            self.logger.warning("supabase package not installed")
            return None
        {% elif memory_provider == "pinecone" %}
        self.logger.info("Initializing Pinecone provider")
        try:
            from pinecone import Pinecone
            
            api_key = os.environ.get("PINECONE_API_KEY")
            environment = os.environ.get("PINECONE_ENVIRONMENT")
            if not api_key or not environment:
                self.logger.warning("Pinecone API key or environment not found")
                return None
            
            pc = Pinecone(api_key=api_key, environment=environment)
            
            # Get or create index
            index_name = "{{package_name}}_memory"
            try:
                index = pc.Index(index_name)
            except:
                pc.create_index(
                    name=index_name,
                    dimension=1536,  # OpenAI embedding dimension
                    metric="cosine"
                )
                index = pc.Index(index_name)
            
            return index
        except ImportError:
            self.logger.warning("pinecone package not installed")
            return None
        {% else %}
        self.logger.info("Using default memory provider")
        return None
        {% endif %}
    
    def _init_logging(self):
        """Initialize logging provider."""
        {% if logging_provider == "none" %}
        self.logger.info("No logging provider configured")
        return None
        {% elif logging_provider == "logfire" %}
        self.logger.info("Initializing LogFire provider")
        try:
            import logfire
            
            api_key = os.environ.get("LOGFIRE_API_KEY")
            if not api_key:
                self.logger.warning("LogFire API key not found")
                return None
            
            logfire.init(api_key)
            return logfire
        except ImportError:
            self.logger.warning("logfire package not installed")
            return None
        {% elif logging_provider == "prometheus" %}
        self.logger.info("Initializing Prometheus provider")
        try:
            from prometheus_client import Counter, Histogram, start_http_server
            
            # Start metrics server on port 8000
            start_http_server(8000)
            
            # Create metrics
            metrics = {
                "requests_total": Counter("agent_requests_total", "Total number of requests"),
                "request_duration": Histogram("agent_request_duration_seconds", "Request duration in seconds"),
                "token_usage": Counter("agent_token_usage_total", "Total number of tokens used", ["type"])
            }
            
            return metrics
        except ImportError:
            self.logger.warning("prometheus_client package not installed")
            return None
        {% elif logging_provider == "langfuse" %}
        self.logger.info("Initializing Langfuse provider")
        try:
            import langfuse
            
            public_key = os.environ.get("LANGFUSE_PUBLIC_KEY")
            secret_key = os.environ.get("LANGFUSE_SECRET_KEY")
            if not public_key or not secret_key:
                self.logger.warning("Langfuse keys not found")
                return None
            
            client = langfuse.Langfuse(
                public_key=public_key,
                secret_key=secret_key
            )
            return client
        except ImportError:
            self.logger.warning("langfuse package not installed")
            return None
        {% else %}
        self.logger.info("Using default logging provider")
        return None
        {% endif %}
    
    def _init_mcp(self):
        """Initialize MCP provider."""
        try:
            from agentscaffold.providers.mcp import load_mcp_servers
            
            # Load MCP servers
            mcp_servers = load_mcp_servers(".")
            if not mcp_servers:
                self.logger.info("No MCP servers found")
                return None
            
            self.logger.info(f"Found {len(mcp_servers)} MCP servers")
            return mcp_servers
        except ImportError:
            self.logger.warning("MCP module not available")
            return None
    
    async def process(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process input data and generate output.
        
        Args:
            input_data: Input data dictionary with 'message' key and optional context
            
        Returns:
            Dictionary with 'response' key and optional metadata
        """
        self.logger.info(f"Processing message: {input_data.get('message', '')}")
        
        # Check for special commands
        message = input_data.get("message", "")
        
        # Handle search queries
        search_query = input_data.get("search_query", "")
        if search_query or any(kw in message.lower() for kw in ["search", "look up", "find"]):
            query = search_query or message
            search_results = await self.search(query)
            
            if search_results:
                formatted_results = "\n".join([
                    f"{i+1}. {result.get('title', 'No title')}\n   {result.get('url', 'No URL')}\n   {result.get('snippet', 'No snippet')}"
                    for i, result in enumerate(search_results[:5])
                ])
                
                response = f"Here are search results for '{query}':\n\n{formatted_results}"
                return {"response": response, "search_results": search_results}
            else:
                return {"response": f"I couldn't find any results for '{query}'.", "search_results": []}
        
        # Handle memory retrieval
        if input_data.get("retrieve_context", False) or any(kw in message.lower() for kw in ["remember", "memory", "previous"]):
            context_query = input_data.get("context_query", message)
            memory_context = await self.remember(context_query)
            
            if memory_context:
                response = f"Here's what I remember:\n\n{memory_context}"
                return {"response": response, "memory_context": memory_context}
            else:
                return {"response": "I don't have any relevant memories.", "memory_context": ""}
        
        # Default to LLM processing
        {% if llm_provider == "none" %}
        # No LLM provider configured, return echo response
        return {"response": f"Received message: {message}", "metadata": {"echo": True}}
        {% elif llm_provider == "openai" %}
        if not self.llm_provider:
            return {"response": "OpenAI provider not initialized", "metadata": {"error": True}}
        
        try:
            # Get memory context if available
            memory_context = await self.remember("")
            
            # Prepare system message
            system_message = f"You are {self.name}, {self.description}."
            if memory_context:
                system_message += f"\n\nPrevious conversation context:\n{memory_context}"
            
            # Call OpenAI API
            completion = await asyncio.to_thread(
                self.llm_provider.chat.completions.create,
                model=os.environ.get("OPENAI_MODEL", "gpt-4o"),
                messages=[
                    {"role": "system", "content": system_message},
                    {"role": "user", "content": message}
                ]
            )
            
            response_text = completion.choices[0].message.content
            
            # Store in memory if requested
            if input_data.get("store_in_memory", True):
                await self.store_memory({
                    "role": "user",
                    "content": message
                })
                await self.store_memory({
                    "role": "assistant",
                    "content": response_text
                })
            
            return {
                "response": response_text,
                "metadata": {
                    "model": completion.model,
                    "usage": completion.usage.model_dump() if hasattr(completion, "usage") else {}
                }
            }
        except Exception as e:
            self.logger.error(f"Error calling OpenAI: {e}")
            return {"response": f"Error processing your message: {str(e)}", "metadata": {"error": True}}
        {% elif llm_provider == "anthropic" %}
        if not self.llm_provider:
            return {"response": "Anthropic provider not initialized", "metadata": {"error": True}}
        
        try:
            # Get memory context if available
            memory_context = await self.remember("")
            
            # Prepare system message
            system_message = f"You are {self.name}, {self.description}."
            if memory_context:
                system_message += f"\n\nPrevious conversation context:\n{memory_context}"
            
            # Call Anthropic API
            response = await self.llm_provider.messages.create(
                model=os.environ.get("ANTHROPIC_MODEL", "claude-3-opus-20240229"),
                max_tokens=1024,
                system=system_message,
                messages=[
                    {"role": "user", "content": message}
                ]
            )
            
            response_text = response.content[0].text
            
            # Store in memory if requested
            if input_data.get("store_in_memory", True):
                await self.store_memory({
                    "role": "user",
                    "content": message
                })
                await self.store_memory({
                    "role": "assistant",
                    "content": response_text
                })
            
            return {
                "response": response_text,
                "metadata": {
                    "model": response.model,
                    "usage": {
                        "input_tokens": response.usage.input_tokens,
                        "output_tokens": response.usage.output_tokens
                    }
                }
            }
        except Exception as e:
            self.logger.error(f"Error calling Anthropic: {e}")
            return {"response": f"Error processing your message: {str(e)}", "metadata": {"error": True}}
        {% elif llm_provider == "ollama" %}
        if not self.llm_provider:
            return {"response": "Ollama provider not initialized", "metadata": {"error": True}}
        
        try:
            client = self.llm_provider["client"]
            base_url = self.llm_provider["base_url"]
            
            # Get memory context if available
            memory_context = await self.remember("")
            
            # Prepare system message
            system_message = f"You are {self.name}, {self.description}."
            if memory_context:
                system_message += f"\n\nPrevious conversation context:\n{memory_context}"
            
            # Call Ollama API
            response = await client.post(
                f"{base_url}/api/generate",
                json={
                    "model": os.environ.get("OLLAMA_MODEL", "llama3"),
                    "prompt": message,
                    "system": system_message
                },
                timeout=60.0
            )
            
            response_json = response.json()
            response_text = response_json.get("response", "")
            
            # Store in memory if requested
            if input_data.get("store_in_memory", True):
                await self.store_memory({
                    "role": "user",
                    "content": message
                })
                await self.store_memory({
                    "role": "assistant",
                    "content": response_text
                })
            
            return {
                "response": response_text,
                "metadata": {
                    "model": response_json.get("model", ""),
                    "total_tokens": response_json.get("total_tokens", 0)
                }
            }
        except Exception as e:
            self.logger.error(f"Error calling Ollama: {e}")
            return {"response": f"Error processing your message: {str(e)}", "metadata": {"error": True}}
        {% else %}
        # No specific LLM provider handling, use default implementation
        return await super().process(input_data)
        {% endif %}
    
    async def search(self, query: str) -> List[Dict[str, Any]]:
        """
        Search for information using the configured search provider.
        
        Args:
            query: Search query
            
        Returns:
            List of search results
        """
        if not query:
            return []
        
        self.logger.info(f"Searching for: {query}")
        
        {% if search_provider == "none" %}
        # No search provider configured
        return []
        {% elif search_provider == "brave" %}
        if not self.search_provider:
            return []
        
        try:
            client = self.search_provider["client"]
            api_key = self.search_provider["api_key"]
            
            url = "https://api.search.brave.com/res/v1/web/search"
            headers = {"Accept": "application/json", "X-Subscription-Token": api_key}
            params = {"q": query, "count": 5}
            
            response = await client.get(url, headers=headers, params=params)
            data = response.json()
            
            if "web" not in data or "results" not in data["web"]:
                return []
            
            results = []
            for result in data["web"]["results"]:
                results.append({
                    "title": result.get("title", ""),
                    "url": result.get("url", ""),
                    "snippet": result.get("description", "")
                })
            
            return results
        except Exception as e:
            self.logger.error(f"Error searching with Brave: {e}")
            return []
        {% elif search_provider == "browserbase" %}
        if not self.search_provider:
            return []
        
        try:
            client = self.search_provider["client"]
            api_key = self.search_provider["api_key"]
            
            url = "https://api.browserbase.com/v1/search"
            headers = {"Authorization": f"Bearer {api_key}"}
            payload = {"query": query, "limit": 5}
            
            response = await client.post(url, headers=headers, json=payload)
            data = response.json()
            
            results = []
            for result in data.get("results", []):
                results.append({
                    "title": result.get("title", ""),
                    "url": result.get("url", ""),
                    "snippet": result.get("content", "")[:200] if result.get("content") else ""
                })
            
            return results
        except Exception as e:
            self.logger.error(f"Error searching with BrowserBase: {e}")
            return []
        {% elif search_provider == "google" %}
        if not self.search_provider:
            return []
        
        try:
            # Run Google search in a thread to avoid blocking
            def perform_search():
                cse_id = os.environ.get("GOOGLE_CSE_ID")
                result = self.search_provider.cse().list(q=query, cx=cse_id, num=5).execute()
                return result.get("items", [])
            
            search_results = await asyncio.to_thread(perform_search)
            
            results = []
            for item in search_results:
                results.append({
                    "title": item.get("title", ""),
                    "url": item.get("link", ""),
                    "snippet": item.get("snippet", "")
                })
            
            return results
        except Exception as e:
            self.logger.error(f"Error searching with Google: {e}")
            return []
        {% else %}
        # No search provider configured, use default implementation
        return await super().search(query)
        {% endif %}
    
    async def remember(self, query: str = "") -> str:
        """
        Retrieve relevant memories based on the query.
        
        Args:
            query: Optional query to filter memories
            
        Returns:
            String containing relevant memories
        """
        self.logger.info(f"Retrieving memories for: {query or 'recent items'}")
        
        {% if memory_provider == "none" %}
        # No memory provider configured
        return ""
        {% elif memory_provider == "chromadb" %}
        if not self.memory_provider:
            return ""
        
        try:
            # Query with empty query returns most recent items
            if not query:
                # Just retrieve the most recent items
                # In a real implementation, this would be more sophisticated
                return "No specific memories found for an empty query."
            
            # Query the collection
            results = self.memory_provider.query(
                query_texts=[query],
                n_results=3
            )
            
            if not results["documents"] or not results["documents"][0]:
                return ""
            
            # Format results
            memories = []
            for i, doc in enumerate(results["documents"][0]):
                metadata = results["metadatas"][0][i] if i < len(results["metadatas"][0]) else {}
                role = metadata.get("role", "")
                if role:
                    memories.append(f"{role.capitalize()}: {doc}")
                else:
                    memories.append(doc)
            
            return "\n\n".join(memories)
        except Exception as e:
            self.logger.error(f"Error retrieving from ChromaDB: {e}")
            return ""
        {% elif memory_provider == "supabase" %}
        if not self.memory_provider:
            return ""
        
        try:
            # Query the database
            if query:
                # Use full-text search if there's a query
                response = await self.memory_provider.table("memories").select("*").textSearch(
                    "content", query
                ).limit(5).execute()
            else:
                # Get most recent entries otherwise
                response = await self.memory_provider.table("memories").select("*").order(
                    "created_at", desc=True
                ).limit(5).execute()
            
            if not response.data:
                return ""
            
            # Format results
            memories = []
            for item in response.data:
                role = item.get("role", "")
                content = item.get("content", "")
                if role and content:
                    memories.append(f"{role.capitalize()}: {content}")
                elif content:
                    memories.append(content)
            
            return "\n\n".join(memories)
        except Exception as e:
            self.logger.error(f"Error retrieving from Supabase: {e}")
            return ""
        {% elif memory_provider == "pinecone" %}
        if not self.memory_provider:
            return ""
        
        try:
            # For Pinecone, we need embeddings
            import numpy as np
            
            # Check if OpenAI is available for embeddings
            try:
                from openai import OpenAI
                
                api_key = os.environ.get("OPENAI_API_KEY")
                if not api_key:
                    self.logger.warning("OpenAI API key not found, cannot generate embeddings")
                    return ""
                
                openai_client = OpenAI(api_key=api_key)
                
                # Generate embedding for query
                if query:
                    embedding_response = openai_client.embeddings.create(
                        input=query,
                        model="text-embedding-3-small"
                    )
                    vector = embedding_response.data[0].embedding
                else:
                    # Use a random vector for empty query (just to get recent items)
                    vector = list(np.random.uniform(-1, 1, 1536))
                    
                # Query Pinecone
                query_response = self.memory_provider.query(
                    vector=vector,
                    top_k=5,
                    include_metadata=True
                )
                
                if not query_response["matches"]:
                    return ""
                
                # Format results
                memories = []
                for match in query_response["matches"]:
                    metadata = match.get("metadata", {})
                    role = metadata.get("role", "")
                    content = metadata.get("content", "")
                    if role and content:
                        memories.append(f"{role.capitalize()}: {content}")
                    elif content:
                        memories.append(content)
                
                return "\n\n".join(memories)
            except ImportError:
                self.logger.warning("OpenAI package not installed, cannot generate embeddings")
                return ""
        except Exception as e:
            self.logger.error(f"Error retrieving from Pinecone: {e}")
            return ""
        {% else %}
        # No memory provider configured, use default implementation
        return await super().remember(query)
        {% endif %}
    
    async def store_memory(self, data: Dict[str, Any]) -> bool:
        """
        Store data in memory.
        
        Args:
            data: Data to store
            
        Returns:
            Success or failure
        """
        if not data:
            return False
        
        self.logger.info("Storing in memory")
        
        {% if memory_provider == "none" %}
        # No memory provider configured
        return False
        {% elif memory_provider == "chromadb" %}
        if not self.memory_provider:
            return False
        
        try:
            # Format document
            role = data.get("role", "")
            content = data.get("content", "")
            
            if not content:
                return False
            
            document = content
            if role:
                document = f"{role}: {content}"
            
            # Generate unique ID
            import uuid
            doc_id = str(uuid.uuid4())
            
            # Add to collection
            self.memory_provider.add(
                documents=[document],
                metadatas=[{k: v for k, v in data.items()}],
                ids=[doc_id]
            )
            
            return True
        except Exception as e:
            self.logger.error(f"Error storing in ChromaDB: {e}")
            return False
        {% elif memory_provider == "supabase" %}
        if not self.memory_provider:
            return False
        
        try:
            # Add timestamp if not provided
            if "created_at" not in data:
                import datetime
                data["created_at"] = datetime.datetime.utcnow().isoformat()
            
            # Insert into database
            response = await self.memory_provider.table("memories").insert(data).execute()
            
            return len(response.data) > 0
        except Exception as e:
            self.logger.error(f"Error storing in Supabase: {e}")
            return False
        {% elif memory_provider == "pinecone" %}
        if not self.memory_provider:
            return False
        
        try:
            # For Pinecone, we need embeddings
            # Check if OpenAI is available for embeddings
            try:
                from openai import OpenAI
                
                api_key = os.environ.get("OPENAI_API_KEY")
                if not api_key:
                    self.logger.warning("OpenAI API key not found, cannot generate embeddings")
                    return False
                
                openai_client = OpenAI(api_key=api_key)
                
                # Get text to embed
                role = data.get("role", "")
                content = data.get("content", "")
                
                if not content:
                    return False
                
                text_to_embed = content
                if role:
                    text_to_embed = f"{role}: {content}"
                
                # Generate embedding
                embedding_response = openai_client.embeddings.create(
                    input=text_to_embed,
                    model="text-embedding-3-small"
                )
                
                vector = embedding_response.data[0].embedding
                
                # Generate ID
                import uuid
                doc_id = str(uuid.uuid4())
                
                # Add timestamp if not provided
                if "timestamp" not in data:
                    import time
                    data["timestamp"] = time.time()
                
                # Upsert to Pinecone
                self.memory_provider.upsert(
                    vectors=[
                        {
                            "id": doc_id,
                            "values": vector,
                            "metadata": {k: v for k, v in data.items() if isinstance(v, (str, int, float, bool))}
                        }
                        }
                    ]
                )
                
                return True
            except ImportError:
                self.logger.warning("OpenAI package not installed, cannot generate embeddings")
                return False
        except Exception as e:
            self.logger.error(f"Error storing in Pinecone: {e}")
            return False
        {% else %}
        # No memory provider configured, use default implementation
        return await super().store_memory(data)
        {% endif %}


# For backward compatibility and direct usage
Agent = {{agent_class_name}}